{
  "id": "caching-strategies",
  "title": "Advanced Caching Strategies",
  "totalLessons": 6,
  "available": true,
  "lessons": [
    {
      "id": 1,
      "title": "Why Do Applications Need Advanced Caching?",
      "goals": [
        "Identify the scaling limits where simple caching strategies break down",
        "Understand how cache hit ratios degrade as application complexity grows",
        "Recognize when caching infrastructure itself becomes the bottleneck"
      ],
      "content": "<div class=\"concept-section\">\n  <h3>When Simple Caching Hits the Wall</h3>\n  <p>Think of caching like a personal assistant's notepad. Initially, your assistant jots down frequently needed information (phone numbers, addresses) on a single notepad for quick reference. This works great when you're managing a small team.<p>But as your organization grows to thousands of employees across multiple offices, that simple notepad approach breaks down. Now you need multiple assistants, each with their own notepad, and they must stay synchronized. When someone's phone number changes, all assistants need to update their notes simultaneously. Some information becomes stale, assistants waste time cross-checking with each other, and you need complex systems to decide which assistant handles which information.<p>The notepad that once made everything faster now requires management overhead that sometimes costs more than just looking things up fresh each time.</p>\n</div>\n<div class=\"concept-section\">\n  <h3>The Scaling Challenges That Break Simple Caching</h3>\n  <ul>\n    <li><strong>Memory limitations:</strong> Single Redis instance maxes out at ~25GB practical limit</li>\n    <li><strong>Cache hit ratio degradation:</strong> Larger datasets mean lower hit rates for same cache size</li>\n    <li><strong>Network bottlenecks:</strong> Cache server CPU and network become saturated</li>\n    <li><strong>Thundering herd:</strong> Popular cache expiries cause simultaneous database hits</li>\n    <li><strong>Cache warming complexity:</strong> Populating distributed caches takes hours after deployment</li>\n    <li><strong>Consistency challenges:</strong> Keeping multiple cache layers synchronized</li>\n  </ul>\n</div>\n<div class=\"concept-section\">\n  <h3>Cache Hit Ratio: The Critical Metric</h3>\n  <p>Cache effectiveness is measured by hit ratio: <strong>(cache hits / total requests) √ó 100</strong></p>\n  <div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;\">\n    <div style=\"background: #e8f5e8; padding: 20px; border-radius: 10px;\">\n      <h4>‚úÖ Healthy Cache Performance</h4>\n      <ul>\n        <li>Hit ratio: 85-95%</li>\n        <li>Sub-5ms cache response time</li>\n        <li>Predictable memory usage growth</li>\n        <li>Rare cache invalidation storms</li>\n      </ul>\n    </div>\n    <div style=\"background: #ffebee; padding: 20px; border-radius: 10px;\">\n      <h4>‚ö†Ô∏è Cache Performance Degradation</h4>\n      <ul>\n        <li>Hit ratio dropping below 70%</li>\n        <li>Cache response time increasing</li>\n        <li>Frequent cache evictions</li>\n        <li>Database load increasing despite caching</li>\n      </ul>\n    </div>\n  </div>\n</div>\n<div class=\"concept-section\">\n  <h3>When Your Cache Becomes the Bottleneck</h3>\n  <p>As applications scale, the cache infrastructure faces its own limits:</p>\n  <ul>\n    <li><strong>CPU saturation:</strong> Redis single-threaded architecture hits limits around 100K ops/second</li>\n    <li><strong>Network bandwidth:</strong> 1Gbps network = ~100MB/s theoretical max for cache responses</li>\n    <li><strong>Memory pressure:</strong> Frequent evictions mean your \"hot\" data keeps getting evicted</li>\n    <li><strong>Connection limits:</strong> Redis defaults to 10K max connections, but practically much lower</li>\n  </ul>\n</div>\n<div class=\"concept-section\">\n  <h3>The Multi-Level Caching Reality</h3>\n  <p>Modern applications don't use a single cache‚Äîthey use a hierarchy:</p>\n  <ol>\n    <li><strong>Browser cache:</strong> Static assets, API responses (minutes to hours)</li>\n    <li><strong>CDN cache:</strong> Geographic distribution (hours to days)</li>\n    <li><strong>Application cache:</strong> In-memory objects (seconds to minutes)</li>\n    <li><strong>Database cache:</strong> Query results and computed data (minutes to hours)</li>\n    <li><strong>Database buffer pool:</strong> Frequently accessed pages (persistent until restart)</li>\n  </ol>\n  <p>Each level has different performance characteristics, size limits, and consistency requirements.</p>\n</div>\n<div class=\"concept-section\">\n  <h3>Real-World Scaling Numbers</h3>\n  <div style=\"background: #f8f9fa; padding: 10px; font-family: monospace; margin: 10px 0;\">\n    Small app (1K users): Single Redis, 95% hit ratio, 1ms latency<br>\n    Medium app (100K users): Redis cluster, 85% hit ratio, 3ms latency<br>\n    Large app (10M users): Multi-tier caching, 78% hit ratio, 8ms latency<br>\n    Massive app (100M+ users): Global cache network, 65% hit ratio, 15ms latency\n  </div>\n  <p>Notice how hit ratios naturally decrease and latencies increase as scale grows‚Äîthis is why advanced strategies become necessary.</p>\n</div>\n<div class=\"exercise\">\n  <h3>üõ†Ô∏è Diagnose the Caching Problem</h3>\n  <p><strong>Scenario:</strong> Your e-commerce platform serves 500K daily active users. You're using a single Redis instance (8GB RAM) to cache product information, user sessions, and shopping carts. Recently, you've noticed:</p>\n  <ul>\n    <li>Cache hit ratio dropped from 92% to 67% over the past month</li>\n    <li>Database CPU spiked during flash sales despite caching</li>\n    <li>Redis memory usage at 95%, frequent evictions</li>\n    <li>Page load times increased from 200ms to 800ms during peak hours</li>\n  </ul>\n  <p><strong>Your task:</strong> Identify the root causes and prioritize which caching problems need immediate attention. What metrics would you monitor to confirm your hypotheses?</p>\n  <p><strong>Consider:</strong> Which types of data should have caching priority? How would you determine the optimal cache size for each data type?</p>\n  <p><em>Bonus: Calculate the approximate cache memory needed to maintain 90% hit ratio for your product catalog of 1M items averaging 2KB each.</em></p>\n</div>"
    },
    {
      "id": 2,
      "title": "Cache Patterns: Aside, Through, and Behind",
      "goals": [
        "Master the three fundamental caching patterns and their use cases",
        "Understand the performance and consistency trade-offs of each approach",
        "Choose the optimal caching pattern based on read/write characteristics"
      ],
      "content": "<div class=\"concept-section\">\n  <h3>The Three Fundamental Caching Patterns</h3>\n  <p>Just like there are different ways to organize a library√É¬¢√¢‚Äö¬¨√¢‚Ç¨some books on display, some in the stacks, some ordered on demand√É¬¢√¢‚Äö¬¨√¢‚Ç¨there are different patterns for organizing your application's cache. Each pattern optimizes for different scenarios and comes with specific trade-offs in complexity, performance, and consistency.</p>\n  <p>The three core patterns determine <strong>when</strong> data gets cached and <strong>who</strong> is responsible for keeping cache and database synchronized.</p>\n</div>\n<div class=\"concept-section\">\n  <h3>Cache-Aside Pattern (Lazy Loading)</h3>\n  <p>The application manages both cache and database directly. On reads: check cache first, load from database if miss, then populate cache.</p>\n  <div style=\"background: #f8f9fa; padding: 10px; font-family: monospace; margin: 10px 0;\">\n    // Pseudocode for cache-aside read<br>\n    function getUser(userId) {<br>\n    &nbsp;&nbsp;user = cache.get(\"user:\" + userId)<br>\n    &nbsp;&nbsp;if (user == null) {<br>\n    &nbsp;&nbsp;&nbsp;&nbsp;user = database.query(\"SELECT * FROM users WHERE id = ?\", userId)<br>\n    &nbsp;&nbsp;&nbsp;&nbsp;cache.set(\"user:\" + userId, user, TTL=300)<br>\n    &nbsp;&nbsp;}<br>\n    &nbsp;&nbsp;return user<br>\n    }\n  </div>\n  <ul>\n    <li>‚úÖ <strong>Simple to implement:</strong> Application controls all cache logic</li>\n    <li>‚úÖ <strong>Resilient:</strong> Cache failures don't break the application</li>\n    <li>‚úÖ <strong>Only caches requested data:</strong> No unnecessary cache pollution</li>\n    <li>‚ùå <strong>Cache miss penalty:</strong> Every miss requires database round-trip</li>\n    <li>‚ùå <strong>Stale data risk:</strong> Updates bypass cache unless manually invalidated</li>\n  </ul>\n  <p><strong>Best for:</strong> Read-heavy workloads with infrequent updates, user profiles, product catalogs</p>\n</div>\n<div class=\"concept-section\">\n  <h3>Write-Through Pattern</h3>\n  <p>All writes go through the cache, which synchronously updates the database. Cache guarantees consistency between cache and database.</p>\n  <div style=\"background: #f8f9fa; padding: 10px; font-family: monospace; margin: 10px 0;\">\n    // Pseudocode for write-through<br>\n    function updateUser(userId, userData) {<br>\n    &nbsp;&nbsp;cache.set(\"user:\" + userId, userData)<br>\n    &nbsp;&nbsp;database.update(\"UPDATE users SET ... WHERE id = ?\", userId, userData)<br>\n    &nbsp;&nbsp;// Both operations must succeed or both fail<br>\n    }\n  </div>\n  <ul>\n    <li>‚úÖ <strong>Always consistent:</strong> Cache and database never out of sync</li>\n    <li>‚úÖ <strong>Read performance:</strong> Popular data always in cache</li>\n    <li>‚úÖ <strong>No cache misses:</strong> For data that's been written recently</li>\n    <li>‚ùå <strong>Write latency:</strong> Every write hits both cache and database</li>\n    <li>‚ùå <strong>Cache pollution:</strong> Caches data that might never be read</li>\n  </ul>\n  <p><strong>Best for:</strong> Applications requiring strong consistency, financial systems, inventory management</p>\n</div>\n<div class=\"concept-section\">\n  <h3>Write-Behind Pattern (Write-Back)</h3>\n  <p>Writes go to cache immediately, database updates happen asynchronously. Provides fastest write performance but eventual consistency.</p>\n  <div style=\"background: #f8f9fa; padding: 10px; font-family: monospace; margin: 10px 0;\">\n    // Pseudocode for write-behind<br>\n    function updateUser(userId, userData) {<br>\n    &nbsp;&nbsp;cache.set(\"user:\" + userId, userData)<br>\n    &nbsp;&nbsp;writeQueue.add({table: \"users\", id: userId, data: userData})<br>\n    &nbsp;&nbsp;return success // Database update happens later<br>\n    }\n  </div>\n  <ul>\n    <li>‚úÖ <strong>Fastest writes:</strong> No database latency on writes</li>\n    <li>‚úÖ <strong>Batch efficiency:</strong> Can batch multiple writes to database</li>\n    <li>‚úÖ <strong>High availability:</strong> Writes succeed even if database is slow</li>\n    <li>‚ùå <strong>Data loss risk:</strong> Cache failure before database sync loses data</li>\n    <li>‚ùå <strong>Complex implementation:</strong> Need reliable async processing</li>\n    <li>‚ùå <strong>Eventual consistency:</strong> Database lags behind cache</li>\n  </ul>\n  <p><strong>Best for:</strong> High-write workloads where eventual consistency is acceptable, gaming leaderboards, activity streams</p>\n</div>\n<div class=\"concept-section\">\n  <h3>Read-Through Pattern</h3>\n  <p>Cache automatically loads data from database on cache misses. Applications only interact with cache, never directly with database.</p>\n  <ul>\n    <li>‚úÖ <strong>Simplified application code:</strong> Cache handles database interaction</li>\n    <li>‚úÖ <strong>Automatic population:</strong> No manual cache warming needed</li>\n    <li>‚ùå <strong>Cache dependency:</strong> Application broken if cache fails</li>\n    <li>‚ùå <strong>Limited flexibility:</strong> Cache must understand database schema</li>\n  </ul>\n  <p><strong>Best for:</strong> Simple applications with straightforward data access patterns</p>\n</div>\n<div class=\"concept-section\">\n  <h3>Choosing the Right Pattern</h3>\n  <div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;\">\n    <div style=\"background: #e3f2fd; padding: 20px; border-radius: 10px;\">\n      <h4>Read-Heavy Applications</h4>\n      <ul>\n        <li>üéØ <strong>Cache-aside</strong> for flexibility</li>\n        <li>üéØ <strong>Read-through</strong> for simplicity</li>\n        <li>Focus on hit ratio optimization</li>\n        <li>Can tolerate some stale data</li>\n      </ul>\n    </div>\n    <div style=\"background: #fff3e0; padding: 20px; border-radius: 10px;\">\n      <h4>Write-Heavy Applications</h4>\n      <ul>\n        <li>üéØ <strong>Write-behind</strong> for performance</li>\n        <li>üéØ <strong>Write-through</strong> for consistency</li>\n        <li>Focus on write latency optimization</li>\n        <li>Consider consistency requirements</li>\n      </ul>\n    </div>\n  </div>\n</div>\n<div class=\"concept-section\">\n  <h3>Hybrid Approaches in Practice</h3>\n  <p>Real applications often combine patterns for different data types:</p>\n  <ul>\n    <li><strong>User profiles:</strong> Cache-aside (read-heavy, infrequent updates)</li>\n    <li><strong>Shopping carts:</strong> Write-through (need consistency for checkout)</li>\n    <li><strong>View counts:</strong> Write-behind (high write volume, eventual consistency OK)</li>\n    <li><strong>Session data:</strong> Write-through (critical for user experience)</li>\n  </ul>\n</div>\n<div class=\"exercise\">\n  <h3>üõ†Ô∏è Design Caching Patterns for Different Scenarios</h3>\n  <p><strong>Scenario:</strong> You're designing caching for a social media platform with these data types:</p>\n  <ol>\n    <li><strong>User posts:</strong> Written once, read frequently by followers, 10:1 read/write ratio</li>\n    <li><strong>Like counts:</strong> Updated constantly, displayed everywhere, 100:1 write/read ratio for updates</li>\n    <li><strong>User notifications:</strong> Written in bursts, read immediately, must be consistent</li>\n    <li><strong>Trending topics:</strong> Computed from aggregations, updated every 5 minutes, read-heavy</li>\n  </ol>\n  <p><strong>Your task:</strong> For each data type, choose the optimal caching pattern and justify your decision. Consider:</p>\n  <ul>\n    <li>Read vs write frequency and performance requirements</li>\n    <li>Consistency needs (immediate vs eventual)</li>\n    <li>Data loss tolerance</li>\n    <li>Implementation complexity</li>\n  </ul>\n  <p><strong>Bonus challenge:</strong> How would you handle the cache for user's home feed, which shows posts from followed users? This requires data from multiple sources and has complex invalidation needs.</p>\n</div>"
    },
    {
      "id": 3,
      "title": "Distributed Caching Architecture",
      "goals": [
        "Understand the two fundamental challenges: data distribution and high availability",
        "Choose between Redis Cluster (scaling) and Redis Sentinel (availability) approaches",
        "Design practical distributed cache architectures for different scaling needs"
      ],
      "content": "<div class=\"concept-section\">\n  <h3>When Single Cache Nodes Aren't Enough</h3>\n  <p>Imagine a popular restaurant that starts with one cash register. As lines get longer, they add a second register, then a third. But now they need a system to ensure all registers know about reservations, loyalty points, and daily specials. This is the challenge of distributed caching√É¬¢√¢‚Äö¬¨√¢‚Ç¨scaling your cache horizontally while maintaining consistency and performance.</p>\n  <p>Single-node cache limitations force you to distribute caching across multiple machines. But distribution introduces two distinct challenges that require different architectural solutions: <strong>data distribution</strong> (how to split data across nodes) and <strong>high availability</strong> (how to handle node failures).</p>\n</div>\n<div class=\"concept-section\">\n  <h3>Data Distribution Strategies</h3>\n  <p>Before diving into specific technologies, let's understand the fundamental approaches for distributing data across cache nodes:</p>\n  <div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;\">\n    <div style=\"background: #e8f5e8; padding: 20px; border-radius: 10px;\">\n      <h4>Hash-Based Distribution</h4>\n      <ul>\n        <li>Apply hash function to cache key</li>\n        <li>Route to node based on hash result</li>\n        <li>‚úÖ Even distribution of data</li>\n        <li>‚ùå Adding nodes requires rehashing</li>\n      </ul>\n    </div>\n    <div style=\"background: #e3f2fd; padding: 20px; border-radius: 10px;\">\n      <h4>Functional Distribution</h4>\n      <ul>\n        <li>Separate nodes by data type or domain</li>\n        <li>User cache, product cache, session cache</li>\n        <li>‚úÖ Independent scaling per domain</li>\n        <li>‚ùå Uneven load distribution</li>\n      </ul>\n    </div>\n  </div>\n</div>\n<div class=\"concept-section\">\n  <h3>Consistent Hashing: Solving the Rehashing Problem</h3>\n  <p>Traditional hash distribution has a major flaw: <code>node = hash(key) % node_count</code> means adding a single node reshuffles ALL data. Consistent hashing solves this by placing both keys and nodes on a hash ring.</p>\n  <div style=\"background: #f8f9fa; padding: 10px; font-family: monospace; margin: 10px 0;\">\n    // Problem with simple hashing:<br>\n    // 3 nodes: key \"user:123\" goes to node hash(\"user:123\") % 3 = node 1<br>\n    // Add 4th node: key \"user:123\" goes to node hash(\"user:123\") % 4 = node 2<br>\n    // Result: Most keys move to different nodes!<br>\n    <br>\n    // Consistent hashing solution:<br>\n    // Keys and nodes both placed on hash ring (0 to 2^32)<br>\n    // Each key goes to first node clockwise from its position<br>\n    // Adding node only affects keys between new node and previous node\n  </div>\n  <ul>\n    <li>‚úÖ <strong>Minimal data movement:</strong> Adding node only affects ~1/N of keys</li>\n    <li>‚úÖ <strong>Virtual nodes:</strong> Each physical node mapped to multiple ring positions for better distribution</li>\n    <li>‚ùå <strong>Implementation complexity:</strong> Need custom client-side routing logic</li>\n  </ul>\n</div>\n<div class=\"concept-section\">\n  <h3>High Availability Approaches</h3>\n  <p>Once you've distributed data, you need to handle node failures. There are two main approaches:</p>\n  <ul>\n    <li><strong>Replication:</strong> Keep copies of data on multiple nodes</li>\n    <li><strong>Automatic failover:</strong> Detect failures and promote backup nodes</li>\n  </ul>\n  <p>The key insight: <strong>scaling</strong> and <strong>availability</strong> are separate concerns that can be solved independently or together.</p>\n</div>\n<div class=\"concept-section\">\n  <h3>Redis Sentinel: High Availability Without Scaling</h3>\n  <p>Redis Sentinel provides high availability for traditional master-replica Redis setups <em>without</em> data partitioning.</p>\n  <div style=\"background: #fff3e0; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n    <h4>üìä Redis Sentinel Architecture</h4>\n    <ul>\n      <li><strong>One master:</strong> Handles all writes, contains complete dataset</li>\n      <li><strong>Multiple replicas:</strong> Receive data from master, handle reads</li>\n      <li><strong>Sentinel processes:</strong> Monitor master health, coordinate failover</li>\n      <li><strong>Automatic failover:</strong> Promote replica to master when master fails</li>\n    </ul>\n  </div>\n  <ul>\n    <li>‚úÖ <strong>Simple application logic:</strong> All data in one logical database</li>\n    <li>‚úÖ <strong>Full Redis features:</strong> Multi-key operations, transactions, Lua scripts</li>\n    <li>‚úÖ <strong>Automatic failover:</strong> ~30 second recovery from master failure</li>\n    <li>‚ùå <strong>Write scaling limited:</strong> Single master handles all writes</li>\n    <li>‚ùå <strong>Memory limited:</strong> Dataset constrained by single master's RAM</li>\n  </ul>\n  <p><strong>Best for:</strong> Applications needing HA but fitting within single-node capacity (~25GB, ~50K writes/sec)</p>\n</div>\n<div class=\"concept-section\">\n  <h3>Redis Cluster: Horizontal Scaling with Built-in HA</h3>\n  <p>Redis Cluster solves the scaling problem by automatically partitioning data across multiple master nodes, with built-in high availability.</p>\n  <div style=\"background: #e3f2fd; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n    <h4>üîÑ Redis Cluster Architecture</h4>\n    <ul>\n      <li><strong>Multiple masters:</strong> Each handles subset of 16,384 hash slots</li>\n      <li><strong>Automatic sharding:</strong> Keys distributed by hash slot</li>\n      <li><strong>Built-in replication:</strong> Each master can have replicas</li>\n      <li><strong>Internal failover:</strong> Cluster promotes replicas automatically</li>\n    </ul>\n  </div>\n  <ul>\n    <li>‚úÖ <strong>Linear scaling:</strong> Add nodes to increase capacity and throughput</li>\n    <li>‚úÖ <strong>Built-in HA:</strong> Survives individual node failures</li>\n    <li>‚úÖ <strong>Automatic rebalancing:</strong> Redistributes slots when adding/removing nodes</li>\n    <li>‚ùå <strong>No cross-slot operations:</strong> Multi-key commands limited to same hash slot</li>\n    <li>‚ùå <strong>Client complexity:</strong> Applications need cluster-aware Redis clients</li>\n    <li>‚ùå <strong>Minimum node count:</strong> Need 6+ nodes for production (3 masters + 3 replicas)</li>\n  </ul>\n  <p><strong>Best for:</strong> Large datasets requiring horizontal scaling (100GB+, 100K+ writes/sec)</p>\n</div>\n<div class=\"concept-section\">\n  <h3>Redis Cluster vs Sentinel: The Decision Matrix</h3>\n  <p><strong>Important:</strong> These are mutually exclusive solutions. Cluster has its own failover mechanism and doesn't use Sentinel.</p>\n  <div style=\"display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px; margin: 20px 0;\">\n    <div style=\"background: #f5f5f5; padding: 15px; border-radius: 8px; text-align: center;\">\n      <h4>Requirement</h4>\n    </div>\n    <div style=\"background: #fff3e0; padding: 15px; border-radius: 8px; text-align: center;\">\n      <h4>Redis Sentinel</h4>\n    </div>\n    <div style=\"background: #e3f2fd; padding: 15px; border-radius: 8px; text-align: center;\">\n      <h4>Redis Cluster</h4>\n    </div>\n    <div style=\"padding: 10px;\"><strong>Scalability needs</strong></div>\n    <div style=\"padding: 10px;\">Single instance sufficient</div>\n    <div style=\"padding: 10px;\">Need to scale beyond single instance</div>\n    <div style=\"padding: 10px;\"><strong>Multi-key operations</strong></div>\n    <div style=\"padding: 10px;\">‚úÖ Full support</div>\n    <div style=\"padding: 10px;\">‚ùå Limited to hash slots</div>\n    <div style=\"padding: 10px;\"><strong>High availability</strong></div>\n    <div style=\"padding: 10px;\">‚úÖ Via Sentinel</div>\n    <div style=\"padding: 10px;\">‚úÖ Built-in</div>\n    <div style=\"padding: 10px;\"><strong>Operational complexity</strong></div>\n    <div style=\"padding: 10px;\">Lower</div>\n    <div style=\"padding: 10px;\">Higher</div>\n  </div>\n</div>\n<div class=\"concept-section\">\n  <h3>Client-Side vs Server-Side Routing</h3>\n  <p>Once you've chosen your distribution strategy, you need to decide how clients find the right cache node:</p>\n  <div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;\">\n    <div style=\"background: #e8f5e8; padding: 20px; border-radius: 10px;\">\n      <h4>Client-Side Routing</h4>\n      <ul>\n        <li>Application calculates target node</li>\n        <li>Direct connections to cache nodes</li>\n        <li>‚úÖ Lower latency (no proxy)</li>\n        <li>‚úÖ Better throughput</li>\n        <li>‚ùå Complex client logic</li>\n        <li>‚ùå Difficult to update routing</li>\n      </ul>\n    </div>\n    <div style=\"background: #f3e5f5; padding: 20px; border-radius: 10px;\">\n      <h4>Server-Side Routing (Proxy)</h4>\n      <ul>\n        <li>Proxy layer handles routing decisions</li>\n        <li>Clients use standard protocols</li>\n        <li>‚úÖ Simple client implementation</li>\n        <li>‚úÖ Easy to update routing logic</li>\n        <li>‚ùå Additional network hop</li>\n        <li>‚ùå Proxy becomes bottleneck</li>\n      </ul>\n    </div>\n  </div>\n  <p><strong>Redis Cluster uses client-side routing</strong> - clients maintain a map of hash slots to nodes and route directly.</p>\n</div>\n<div class=\"concept-section\">\n  <h3>Practical Architecture Patterns</h3>\n  <ol>\n    <li><strong>Start Simple:</strong> Single Redis with monitoring ‚Üí Add Sentinel for HA ‚Üí Migrate to Cluster for scale</li>\n    <li><strong>Functional + Horizontal:</strong> Separate cache clusters by domain (users, products, sessions)</li>\n    <li><strong>Tiered Caching:</strong> L1 application cache ‚Üí L2 Redis Cluster ‚Üí L3 database</li>\n    <li><strong>Geographic Distribution:</strong> Regional cache clusters with async replication</li>\n  </ol>\n</div>\n<div class=\"exercise\">\n  <h3>üõ†Ô∏è Choose Your Distributed Caching Strategy</h3>\n  <p><strong>Scenario:</strong> You need to choose caching architecture for three different applications:</p>\n  <div style=\"background: #f8f9fa; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n    <h4>Application A: Financial Trading Platform</h4>\n    <ul>\n      <li>5GB of market data, updated frequently</li>\n      <li>10K trades/second during market hours</li>\n      <li>Requires multi-key transactions for trade validation</li>\n      <li>99.99% availability requirement</li>\n    </ul>\n    \n    <h4>Application B: Social Media Platform</h4>\n    <ul>\n      <li>500GB of user posts and profiles</li>\n      <li>100K writes/second globally</li>\n      <li>Mostly single-key lookups (user profiles, individual posts)</li>\n      <li>Can tolerate brief inconsistencies</li>\n    </ul>\n    \n    <h4>Application C: E-commerce Site</h4>\n    <ul>\n      <li>50GB product catalog + 20GB user sessions</li>\n      <li>30K mixed read/write operations/second</li>\n      <li>Shopping cart operations need consistency</li>\n      <li>Global presence across 3 regions</li>\n    </ul>\n  </div>\n  <p><strong>Your task:</strong> For each application, decide:</p>\n  <ol>\n    <li>Redis Sentinel or Redis Cluster (explain why)</li>\n    <li>Data distribution strategy</li>\n    <li>How you'd handle the high availability requirement</li>\n    <li>Client-side or server-side routing</li>\n  </ol>\n  <p><strong>Consider:</strong> What happens when your chosen solution hits its limits? What would be your migration path to the next level of scale?</p>\n</div>"
    },
    {
      "id": 4,
      "title": "Cache Invalidation and Consistency Strategies",
      "goals": [
        "Master the hardest problem in computer science: cache invalidation",
        "Implement strategies to prevent cache stampedes and thundering herds",
        "Design multi-level cache hierarchies with proper consistency guarantees"
      ],
      "content": "<div class=\"concept-section\">\n  <h3>The Two Hard Problems in Computer Science</h3>\n  <p>Phil Karlton famously said: \"There are only two hard things in Computer Science: cache invalidation and naming things.\" Cache invalidation is hard because it requires coordinating state across distributed systems while maintaining performance. Get it wrong, and users see stale data. Get it too aggressive, and you lose all caching benefits.</p>\n  <p>Cache invalidation becomes exponentially more complex with multiple cache layers, geographic distribution, and high-frequency updates. The challenge isn't just knowing <strong>when</strong> to invalidate, but <strong>how</strong> to do it efficiently across your entire cache hierarchy.</p>\n</div>\n<div class=\"concept-section\">\n  <h3>Time-Based vs Event-Based Invalidation</h3>\n  <div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;\">\n    <div style=\"background: #e8f5e8; padding: 20px; border-radius: 10px;\">\n      <h4>Time-Based (TTL)</h4>\n      <ul>\n        <li>‚úÖ <strong>Simple implementation:</strong> Set expiration on cache write</li>\n        <li>‚úÖ <strong>Prevents infinite staleness:</strong> Data refreshes automatically</li>\n        <li>‚úÖ <strong>No coordination needed:</strong> Each cache node independent</li>\n        <li>‚ùå <strong>Unnecessary refreshes:</strong> Expires even if data unchanged</li>\n        <li>‚ùå <strong>Stale data windows:</strong> Data can be stale for entire TTL period</li>\n      </ul>\n    </div>\n    <div style=\"background: #e3f2fd; padding: 20px; border-radius: 10px;\">\n      <h4>Event-Based</h4>\n      <ul>\n        <li>‚úÖ <strong>Immediate consistency:</strong> Cache updates on data changes</li>\n        <li>‚úÖ <strong>Efficient:</strong> Only invalidates when necessary</li>\n        <li>‚úÖ <strong>Predictable freshness:</strong> Know exactly when cache was last updated</li>\n        <li>‚ùå <strong>Complex coordination:</strong> Need reliable event delivery</li>\n        <li>‚ùå <strong>Failure scenarios:</strong> Missed events cause permanent staleness</li>\n      </ul>\n    </div>\n  </div>\n</div>\n<div class=\"concept-section\">\n  <h3>Cache Stampede: The Thundering Herd Problem</h3>\n  <p>When a popular cache entry expires, multiple requests simultaneously hit the database to regenerate the same data. This can overwhelm your database and create a cascading failure.</p>\n  <div style=\"background: #f8f9fa; padding: 10px; font-family: monospace; margin: 10px 0;\">\n    // Problem: Multiple threads all try to regenerate expired cache<br>\n    popular_data = cache.get(\"trending_posts\")<br>\n    if (popular_data == null) {<br>\n    &nbsp;&nbsp;// 1000 concurrent requests all hit database simultaneously!<br>\n    &nbsp;&nbsp;popular_data = database.expensiveQuery()<br>\n    &nbsp;&nbsp;cache.set(\"trending_posts\", popular_data, 300)<br>\n    }\n  </div>\n  <p><strong>Solutions to prevent cache stampede:</strong></p>\n  <ul>\n    <li><strong>Lock-based approach:</strong> First request gets lock, others wait for result</li>\n    <li><strong>Probabilistic early expiration:</strong> Randomly refresh before TTL expires</li>\n    <li><strong>Background refresh:</strong> Async process keeps cache warm</li>\n    <li><strong>Stale-while-revalidate:</strong> Serve stale data while regenerating fresh data</li>\n  </ul>\n</div>\n<div class=\"concept-section\">\n  <h3>Advanced Invalidation Strategies</h3>\n  <p><strong>Cache Tags:</strong> Group related cache entries for batch invalidation</p>\n  <div style=\"background: #f8f9fa; padding: 10px; font-family: monospace; margin: 10px 0;\">\n    // Tag cache entries by user and content type<br>\n    cache.set(\"user:123:profile\", userData, tags=[\"user:123\", \"profile\"])<br>\n    cache.set(\"user:123:posts\", userPosts, tags=[\"user:123\", \"posts\"])<br>\n    <br>\n    // Invalidate all user:123 related data at once<br>\n    cache.invalidateByTag(\"user:123\")\n  </div>\n  <p><strong>Dependency-based invalidation:</strong> Define relationships between cached data</p>\n  <ul>\n    <li>User profile change ‚Üí invalidate user posts cache</li>\n    <li>Product update ‚Üí invalidate category listing cache</li>\n    <li>Configuration change ‚Üí invalidate application settings cache</li>\n  </ul>\n</div>\n<div class=\"concept-section\">\n  <h3>Multi-Level Cache Consistency</h3>\n  <p>With CDN ‚Üí Application Cache ‚Üí Database Cache hierarchy, invalidation must propagate through all levels:</p>\n  <ol>\n    <li><strong>Write-through propagation:</strong> Update flows from inner cache to outer cache</li>\n    <li><strong>Reverse invalidation:</strong> Database changes trigger outward invalidation</li>\n    <li><strong>Eventually consistent:</strong> Accept temporary inconsistency between levels</li>\n    <li><strong>Version-based:</strong> Include version numbers to detect stale data</li>\n  </ol>\n</div>\n<div class=\"concept-section\">\n  <h3>Cache Coherence Patterns</h3>\n  <div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;\">\n    <div style=\"background: #fff3e0; padding: 20px; border-radius: 10px;\">\n      <h4>Write-Invalidate</h4>\n      <ul>\n        <li>Writer removes data from other caches</li>\n        <li>‚úÖ Reduces network traffic for updates</li>\n        <li>‚ùå Cache misses after invalidation</li>\n        <li>Best for: Infrequent writes, expensive data generation</li>\n      </ul>\n    </div>\n    <div style=\"background: #f3e5f5; padding: 20px; border-radius: 10px;\">\n      <h4>Write-Update</h4>\n      <ul>\n        <li>Writer pushes new data to other caches</li>\n        <li>‚úÖ No cache misses after updates</li>\n        <li>‚ùå High network overhead for updates</li>\n        <li>Best for: Frequent reads, small data payloads</li>\n      </ul>\n    </div>\n  </div>\n</div>\n<div class=\"concept-section\">\n  <h3>Implementing Stale-While-Revalidate</h3>\n  <div style=\"background: #f8f9fa; padding: 10px; font-family: monospace; margin: 10px 0;\">\n    function getWithStaleWhileRevalidate(key) {<br>\n    &nbsp;&nbsp;cached = cache.get(key)<br>\n    &nbsp;&nbsp;if (cached && !cached.isExpired()) {<br>\n    &nbsp;&nbsp;&nbsp;&nbsp;return cached.data<br>\n    &nbsp;&nbsp;}<br>\n    &nbsp;&nbsp;<br>\n    &nbsp;&nbsp;if (cached && cached.isStale()) {<br>\n    &nbsp;&nbsp;&nbsp;&nbsp;// Serve stale data immediately<br>\n    &nbsp;&nbsp;&nbsp;&nbsp;asyncRefresh(key)  // Background refresh<br>\n    &nbsp;&nbsp;&nbsp;&nbsp;return cached.data<br>\n    &nbsp;&nbsp;}<br>\n    &nbsp;&nbsp;<br>\n    &nbsp;&nbsp;// Cache miss - synchronous refresh<br>\n    &nbsp;&nbsp;return refreshAndCache(key)<br>\n    }\n  </div>\n  <p>This pattern provides the best user experience: fast responses with eventual consistency.</p>\n</div>\n<div class=\"concept-section\">\n  <h3>Cache Invalidation Anti-Patterns</h3>\n  <div style=\"background: #ffebee; padding: 20px; border-radius: 10px; margin: 20px 0;\">\n    <h4>‚ö†Ô∏è Avoid These Common Mistakes</h4>\n    <ul>\n      <li><strong>Invalidate-everything approach:</strong> Clearing entire cache on any update</li>\n      <li><strong>Synchronous invalidation:</strong> Making user requests wait for cache clearing</li>\n      <li><strong>Ignore invalidation failures:</strong> Not handling cases where invalidation fails</li>\n      <li><strong>Circular dependencies:</strong> Cache A invalidates B, which invalidates A</li>\n      <li><strong>Over-granular invalidation:</strong> Too many small cache keys creating overhead</li>\n    </ul>\n  </div>\n</div>\n<div class=\"concept-section\">\n  <h3>Monitoring Cache Consistency</h3>\n  <p>Key metrics to track cache invalidation effectiveness:</p>\n  <ul>\n    <li><strong>Invalidation latency:</strong> Time from data change to cache update</li>\n    <li><strong>Invalidation success rate:</strong> Percentage of successful invalidations</li>\n    <li><strong>Stale data detection:</strong> Comparing cache vs database for consistency</li>\n    <li><strong>Cascade failure rate:</strong> How often invalidation causes downstream problems</li>\n  </ul>\n</div>\n<div class=\"exercise\">\n  <h3>üõ†Ô∏è Design Cache Invalidation Strategy</h3>\n  <p><strong>Scenario:</strong> You're building a news aggregation platform with these components:</p>\n  <ul>\n    <li><strong>Article content:</strong> Cached for 1 hour, but needs immediate updates for breaking news</li>\n    <li><strong>User recommendations:</strong> Expensive ML computation, cached for 4 hours, personalized per user</li>\n    <li><strong>Trending topics:</strong> Updated every 5 minutes, affects multiple page sections</li>\n    <li><strong>User reading history:</strong> High write frequency, affects recommendations and trending</li>\n  </ul>\n  <p><strong>Cache hierarchy:</strong> CDN (global) ‚Üí App Cache (regional) ‚Üí Database Cache (local)</p>\n  <p><strong>Your task:</strong> Design an invalidation strategy that addresses:</p>\n  <ol>\n    <li>How to handle breaking news updates across all cache levels?</li>\n    <li>What happens when ML recommendation service is down?</li>\n    <li>How to prevent stampede when trending topics refresh?</li>\n    <li>How to track that a user's reading history properly invalidates their recommendations?</li>\n  </ol>\n  <p><strong>Consider:</strong> Your CDN has a 30-second propagation delay. How does this affect your invalidation strategy? What data can tolerate this delay?</p>\n  <p><em>Bonus: Design a monitoring dashboard showing cache consistency health across all levels.</em></p>\n</div>"
    }
  ]
}